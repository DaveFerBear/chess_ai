{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess.pgn\n",
    "import time\n",
    "import sys\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOARD_SPACES = 64\n",
    "ROWS = 8\n",
    "COLS = 8\n",
    "\n",
    "letter_lookup = {\n",
    "    'a':0,\n",
    "    'b':1,\n",
    "    'c':2,\n",
    "    'd':3,\n",
    "    'e':4,\n",
    "    'f':5,\n",
    "    'g':6,\n",
    "    'h':7\n",
    "}\n",
    "piece_lookup = {\n",
    "    'r':1,\n",
    "    'n':2,\n",
    "    'b':3,\n",
    "    'q':4,\n",
    "    'k':5,\n",
    "    'p':6,\n",
    "    'R':-1,\n",
    "    'N':-2,\n",
    "    'B':-3,\n",
    "    'Q':-4,\n",
    "    'K':-5,\n",
    "    'P':-6,\n",
    "    '.':0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [-1.,  0.,  0.,  0.,  0.,  0.,  0., -1.]],\n",
       "\n",
       "       [[ 0.,  1.,  0.,  0.,  0.,  0.,  1.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0., -1.,  0.,  0.,  0.,  0., -1.,  0.]],\n",
       "\n",
       "       [[ 0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0., -1.,  0.,  0., -1.,  0.,  0.]],\n",
       "\n",
       "       [[ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., -1.,  0.,  0.,  0.,  0.]],\n",
       "\n",
       "       [[ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0., -1.,  0.,  0.,  0.]],\n",
       "\n",
       "       [[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [-1., -1., -1., -1., -1., -1., -1., -1.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]],\n",
       "\n",
       "       [[ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.]]])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Format of Dataset\n",
    "# One chanel per piece\n",
    "# One additional channel showing whose turn it is\n",
    "# Referenced from https://int8.io/chess-position-evaluation-with-convolutional-neural-networks-in-julia/\n",
    "\n",
    "dataset_x = []\n",
    "dataset_y = []\n",
    "posns = []\n",
    "\n",
    "pgn = open(\"datasets/AllGames.pgn\")\n",
    "game = chess.pgn.read_game(pgn)\n",
    "board = game.board()\n",
    "black_or_white = True\n",
    "count = 0\n",
    "\n",
    "while game != None:\n",
    "#     print(count)\n",
    "    count += 1\n",
    "    if count >= 100:\n",
    "        break\n",
    "\n",
    "    for move in game.mainline_moves():\n",
    "        dp = []\n",
    "        \n",
    "        # Create a one-hot label for the move\n",
    "        label = np.zeros(BOARD_SPACES*2)\n",
    "        pos1 = str(move)[:2]\n",
    "        pos1 = ROWS*letter_lookup[pos1[0]] + int(pos1[1])-1\n",
    "        label[pos1] = 1\n",
    "        pos2 = str(move)[2:]\n",
    "        pos2 = ROWS*letter_lookup[pos2[0]] + int(pos2[1])-1\n",
    "        label[(BOARD_SPACES) + pos2] = 1\n",
    "        \n",
    "        posns.append((pos1, pos2))\n",
    "        \n",
    "        # Turn the board into a vector\n",
    "        strboard = str(board)\n",
    "        board_data = np.zeros((7, 8, 8))\n",
    "        idx = 0\n",
    "        for char in strboard:\n",
    "            if char in piece_lookup:\n",
    "                if char != '.':\n",
    "                    board_data[abs(piece_lookup[char])-1][int(idx/COLS)][idx%ROWS] \\\n",
    "                        = piece_lookup[char]/abs(piece_lookup[char])\n",
    "                idx += 1\n",
    "        # Fill the final channel with ones if white\n",
    "        if black_or_white == True:\n",
    "            board_data[6].fill(1)\n",
    "                \n",
    "        dataset_x.append(board_data)\n",
    "        dataset_y.append(label)  \n",
    "        black_or_white = not black_or_white\n",
    "        board.push(move)\n",
    "    game = chess.pgn.read_game(pgn)\n",
    "\n",
    "dataset_x = np.stack(dataset_x)\n",
    "dataset_y = np.stack(dataset_y)\n",
    "dataset_x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7110, 7, 8, 8) 128\n"
     ]
    }
   ],
   "source": [
    "inshape = dataset_x[0].shape\n",
    "outshape = dataset_y[0].size\n",
    "print(dataset_x.shape, outshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD, Adadelta, Adam, RMSprop\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(128, kernel_size=(2, 2),\n",
    "                 input_shape=inshape))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=(2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(1024))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(1024))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(outshape, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "tensorboard = TensorBoard(log_dir='logs')\n",
    "filepath = 'models/best_model.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, verbose=1,\n",
    "                             save_best_only=True, mode='max')\n",
    "callbacks = [tensorboard, checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "7110/7110 [==============================] - 11s 2ms/step - loss: 9.5310 - acc: 0.0042\n",
      "Epoch 2/2000\n",
      "7110/7110 [==============================] - 11s 2ms/step - loss: 9.4851 - acc: 4.2194e-04\n",
      "Epoch 3/2000\n",
      "7110/7110 [==============================] - 12s 2ms/step - loss: 9.4735 - acc: 0.0000e+00\n",
      "Epoch 4/2000\n",
      "7110/7110 [==============================] - 14s 2ms/step - loss: 9.4590 - acc: 0.0028\n",
      "Epoch 5/2000\n",
      "7110/7110 [==============================] - 12s 2ms/step - loss: 9.4401 - acc: 0.0035\n",
      "Epoch 6/2000\n",
      "7110/7110 [==============================] - 11s 2ms/step - loss: 9.4160 - acc: 0.0013\n",
      "Epoch 7/2000\n",
      "7110/7110 [==============================] - 12s 2ms/step - loss: 9.3861 - acc: 0.0018\n",
      "Epoch 8/2000\n",
      "7110/7110 [==============================] - 13s 2ms/step - loss: 9.3572 - acc: 0.0021\n",
      "Epoch 9/2000\n",
      "7110/7110 [==============================] - 12s 2ms/step - loss: 9.3322 - acc: 0.0027\n",
      "Epoch 10/2000\n",
      "7110/7110 [==============================] - 13s 2ms/step - loss: 9.3065 - acc: 0.0032\n",
      "Epoch 11/2000\n",
      "7110/7110 [==============================] - 13s 2ms/step - loss: 9.2848 - acc: 0.0051\n",
      "Epoch 12/2000\n",
      "7110/7110 [==============================] - 11s 2ms/step - loss: 9.2670 - acc: 0.0041\n",
      "Epoch 13/2000\n",
      "7110/7110 [==============================] - 13s 2ms/step - loss: 9.2486 - acc: 0.0077\n",
      "Epoch 14/2000\n",
      "7110/7110 [==============================] - 11s 2ms/step - loss: 9.2336 - acc: 0.0062\n",
      "Epoch 15/2000\n",
      "7110/7110 [==============================] - 11s 2ms/step - loss: 9.2257 - acc: 0.0060\n",
      "Epoch 16/2000\n",
      "7110/7110 [==============================] - 12s 2ms/step - loss: 9.2115 - acc: 0.0068\n",
      "Epoch 17/2000\n",
      "7110/7110 [==============================] - 13s 2ms/step - loss: 9.2066 - acc: 0.0076\n",
      "Epoch 18/2000\n",
      "7110/7110 [==============================] - 12s 2ms/step - loss: 9.1962 - acc: 0.0076\n",
      "Epoch 19/2000\n",
      "7110/7110 [==============================] - 11s 2ms/step - loss: 9.1910 - acc: 0.0082\n",
      "Epoch 20/2000\n",
      "7110/7110 [==============================] - 13s 2ms/step - loss: 9.1871 - acc: 0.0083\n",
      "Epoch 21/2000\n",
      "7110/7110 [==============================] - 14s 2ms/step - loss: 9.1821 - acc: 0.0084\n",
      "Epoch 22/2000\n",
      "7110/7110 [==============================] - 16s 2ms/step - loss: 9.1822 - acc: 0.0072\n",
      "Epoch 23/2000\n",
      "7110/7110 [==============================] - 14s 2ms/step - loss: 9.1744 - acc: 0.0101\n",
      "Epoch 24/2000\n",
      "7110/7110 [==============================] - 14s 2ms/step - loss: 9.1690 - acc: 0.0084\n",
      "Epoch 25/2000\n",
      "7110/7110 [==============================] - 13s 2ms/step - loss: 9.1661 - acc: 0.0098\n",
      "Epoch 26/2000\n",
      "7110/7110 [==============================] - 13s 2ms/step - loss: 9.1619 - acc: 0.0093\n",
      "Epoch 27/2000\n",
      "7110/7110 [==============================] - 12s 2ms/step - loss: 9.1599 - acc: 0.0103\n",
      "Epoch 28/2000\n",
      "7110/7110 [==============================] - 12s 2ms/step - loss: 9.1577 - acc: 0.0090\n",
      "Epoch 29/2000\n",
      "7110/7110 [==============================] - 12s 2ms/step - loss: 9.1562 - acc: 0.0091\n",
      "Epoch 30/2000\n",
      "7110/7110 [==============================] - 12s 2ms/step - loss: 9.1529 - acc: 0.0083\n",
      "Epoch 31/2000\n",
      "7110/7110 [==============================] - 12s 2ms/step - loss: 9.1494 - acc: 0.0113\n",
      "Epoch 32/2000\n",
      "7110/7110 [==============================] - 14s 2ms/step - loss: 9.1491 - acc: 0.0091\n",
      "Epoch 33/2000\n",
      "7110/7110 [==============================] - 14s 2ms/step - loss: 9.1452 - acc: 0.0105\n",
      "Epoch 34/2000\n",
      "7110/7110 [==============================] - 13s 2ms/step - loss: 9.1442 - acc: 0.0097\n",
      "Epoch 35/2000\n",
      "7110/7110 [==============================] - 11s 2ms/step - loss: 9.1441 - acc: 0.0118\n",
      "Epoch 36/2000\n",
      "7110/7110 [==============================] - 15s 2ms/step - loss: 9.1419 - acc: 0.0094\n",
      "Epoch 37/2000\n",
      "7110/7110 [==============================] - 19s 3ms/step - loss: 9.1401 - acc: 0.0113\n",
      "Epoch 38/2000\n",
      "7110/7110 [==============================] - 16s 2ms/step - loss: 9.1433 - acc: 0.0089\n",
      "Epoch 39/2000\n",
      "7110/7110 [==============================] - 12s 2ms/step - loss: 9.1400 - acc: 0.0114\n",
      "Epoch 40/2000\n",
      "7110/7110 [==============================] - 17s 2ms/step - loss: 9.1403 - acc: 0.0105\n",
      "Epoch 41/2000\n",
      "7110/7110 [==============================] - 14s 2ms/step - loss: 9.1382 - acc: 0.0100\n",
      "Epoch 42/2000\n",
      "7110/7110 [==============================] - 14s 2ms/step - loss: 9.1388 - acc: 0.0111\n",
      "Epoch 43/2000\n",
      "7110/7110 [==============================] - 12s 2ms/step - loss: 9.1397 - acc: 0.0090\n",
      "Epoch 44/2000\n",
      "7110/7110 [==============================] - 11s 2ms/step - loss: 9.1397 - acc: 0.0105\n",
      "Epoch 45/2000\n",
      "7110/7110 [==============================] - 11s 2ms/step - loss: 9.1369 - acc: 0.0086\n",
      "Epoch 46/2000\n",
      "7110/7110 [==============================] - 12s 2ms/step - loss: 9.1367 - acc: 0.0118\n",
      "Epoch 47/2000\n",
      "7110/7110 [==============================] - 11s 2ms/step - loss: 9.1340 - acc: 0.0101\n",
      "Epoch 48/2000\n",
      "7110/7110 [==============================] - 10s 1ms/step - loss: 9.1334 - acc: 0.0113\n",
      "Epoch 49/2000\n",
      "7110/7110 [==============================] - 10s 1ms/step - loss: 9.1335 - acc: 0.0093\n",
      "Epoch 50/2000\n",
      "7110/7110 [==============================] - 10s 1ms/step - loss: 9.1322 - acc: 0.0113\n",
      "Epoch 51/2000\n",
      "7110/7110 [==============================] - 10s 1ms/step - loss: 9.1322 - acc: 0.0110\n",
      "Epoch 52/2000\n",
      "7110/7110 [==============================] - 10s 1ms/step - loss: 9.1324 - acc: 0.0104\n",
      "Epoch 53/2000\n",
      "7110/7110 [==============================] - 10s 1ms/step - loss: 9.1316 - acc: 0.0105\n",
      "Epoch 54/2000\n",
      "7110/7110 [==============================] - 10s 1ms/step - loss: 9.1320 - acc: 0.0105\n",
      "Epoch 55/2000\n",
      "7110/7110 [==============================] - 10s 1ms/step - loss: 9.1314 - acc: 0.0098\n",
      "Epoch 56/2000\n",
      "7110/7110 [==============================] - 10s 1ms/step - loss: 9.1311 - acc: 0.0118\n",
      "Epoch 57/2000\n",
      "7110/7110 [==============================] - 10s 1ms/step - loss: 9.1304 - acc: 0.0087\n",
      "Epoch 58/2000\n",
      "7110/7110 [==============================] - 10s 1ms/step - loss: 9.1299 - acc: 0.0104\n",
      "Epoch 59/2000\n",
      "7110/7110 [==============================] - 10s 1ms/step - loss: 9.1304 - acc: 0.0100\n",
      "Epoch 60/2000\n",
      "7110/7110 [==============================] - 10s 1ms/step - loss: 9.1290 - acc: 0.0121\n",
      "Epoch 61/2000\n",
      "7110/7110 [==============================] - 10s 1ms/step - loss: 9.1278 - acc: 0.0096\n",
      "Epoch 62/2000\n",
      "7110/7110 [==============================] - 10s 1ms/step - loss: 9.1283 - acc: 0.0113\n",
      "Epoch 63/2000\n",
      "7110/7110 [==============================] - 10s 1ms/step - loss: 9.1269 - acc: 0.0098\n",
      "Epoch 64/2000\n",
      "7110/7110 [==============================] - 10s 1ms/step - loss: 9.1276 - acc: 0.0110\n",
      "Epoch 65/2000\n",
      "7110/7110 [==============================] - 10s 1ms/step - loss: 9.1278 - acc: 0.0096\n",
      "Epoch 66/2000\n",
      "7110/7110 [==============================] - 10s 1ms/step - loss: 9.1266 - acc: 0.0103\n",
      "Epoch 67/2000\n",
      "7110/7110 [==============================] - 10s 1ms/step - loss: 9.1311 - acc: 0.0111\n",
      "Epoch 68/2000\n",
      "7110/7110 [==============================] - 10s 1ms/step - loss: 9.1341 - acc: 0.0105\n",
      "Epoch 69/2000\n",
      "7110/7110 [==============================] - 10s 1ms/step - loss: 9.1413 - acc: 0.0111\n",
      "Epoch 70/2000\n",
      "7110/7110 [==============================] - 10s 1ms/step - loss: 9.1386 - acc: 0.0083\n",
      "Epoch 71/2000\n",
      "7110/7110 [==============================] - 10s 1ms/step - loss: 9.1339 - acc: 0.0096\n",
      "Epoch 72/2000\n",
      "7110/7110 [==============================] - 10s 1ms/step - loss: 9.1282 - acc: 0.0103\n",
      "Epoch 73/2000\n",
      "7110/7110 [==============================] - 10s 1ms/step - loss: 9.1264 - acc: 0.0115\n",
      "Epoch 74/2000\n",
      "7110/7110 [==============================] - 10s 1ms/step - loss: 9.1238 - acc: 0.0091\n",
      "Epoch 75/2000\n",
      "7110/7110 [==============================] - 10s 1ms/step - loss: 9.1221 - acc: 0.0118\n",
      "Epoch 76/2000\n",
      "7110/7110 [==============================] - 10s 1ms/step - loss: 9.1226 - acc: 0.0100\n",
      "Epoch 77/2000\n",
      "7110/7110 [==============================] - 10s 1ms/step - loss: 9.1224 - acc: 0.0107\n",
      "Epoch 78/2000\n",
      "7110/7110 [==============================] - 10s 1ms/step - loss: 9.1220 - acc: 0.0110\n",
      "Epoch 79/2000\n",
      "7110/7110 [==============================] - 10s 1ms/step - loss: 9.1218 - acc: 0.0111\n",
      "Epoch 80/2000\n",
      "7110/7110 [==============================] - 10s 1ms/step - loss: 9.1225 - acc: 0.0115\n",
      "Epoch 81/2000\n",
      "7110/7110 [==============================] - 10s 1ms/step - loss: 9.1223 - acc: 0.0097\n",
      "Epoch 82/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7110/7110 [==============================] - 13s 2ms/step - loss: 9.1222 - acc: 0.0094\n",
      "Epoch 83/2000\n",
      "7110/7110 [==============================] - 12s 2ms/step - loss: 9.1211 - acc: 0.0100\n",
      "Epoch 84/2000\n",
      "7110/7110 [==============================] - 11s 2ms/step - loss: 9.1218 - acc: 0.0110\n",
      "Epoch 85/2000\n",
      "7110/7110 [==============================] - 12s 2ms/step - loss: 9.1222 - acc: 0.0111\n",
      "Epoch 86/2000\n",
      "7110/7110 [==============================] - 12s 2ms/step - loss: 9.1231 - acc: 0.0117\n",
      "Epoch 87/2000\n",
      "7110/7110 [==============================] - 12s 2ms/step - loss: 9.1213 - acc: 0.0110\n",
      "Epoch 88/2000\n",
      "7110/7110 [==============================] - 13s 2ms/step - loss: 9.1214 - acc: 0.0100\n",
      "Epoch 89/2000\n",
      "7110/7110 [==============================] - 13s 2ms/step - loss: 9.1207 - acc: 0.0118\n",
      "Epoch 90/2000\n",
      "7110/7110 [==============================] - 12s 2ms/step - loss: 9.1217 - acc: 0.0100\n",
      "Epoch 91/2000\n",
      "7110/7110 [==============================] - 15s 2ms/step - loss: 9.1202 - acc: 0.0101\n",
      "Epoch 92/2000\n",
      "7110/7110 [==============================] - 12s 2ms/step - loss: 9.1221 - acc: 0.0124\n",
      "Epoch 93/2000\n",
      "7110/7110 [==============================] - 11s 1ms/step - loss: 9.1219 - acc: 0.0097\n",
      "Epoch 94/2000\n",
      "7110/7110 [==============================] - 14s 2ms/step - loss: 9.1245 - acc: 0.0113\n",
      "Epoch 95/2000\n",
      "7110/7110 [==============================] - 12s 2ms/step - loss: 9.1249 - acc: 0.0107\n",
      "Epoch 96/2000\n",
      "7110/7110 [==============================] - 15s 2ms/step - loss: 9.1291 - acc: 0.0114\n",
      "Epoch 97/2000\n",
      "7110/7110 [==============================] - 14s 2ms/step - loss: 9.1258 - acc: 0.0090: 1s - loss: 9.1215 - \n",
      "Epoch 98/2000\n",
      "7110/7110 [==============================] - 11s 1ms/step - loss: 9.1258 - acc: 0.0111\n",
      "Epoch 99/2000\n",
      "7110/7110 [==============================] - 10s 1ms/step - loss: 9.1221 - acc: 0.0114\n",
      "Epoch 100/2000\n",
      "7110/7110 [==============================] - 11s 2ms/step - loss: 9.1197 - acc: 0.0113\n",
      "Epoch 101/2000\n",
      "7110/7110 [==============================] - 14s 2ms/step - loss: 9.1187 - acc: 0.0094\n",
      "Epoch 102/2000\n",
      "7110/7110 [==============================] - 10s 1ms/step - loss: 9.1180 - acc: 0.0110\n",
      "Epoch 103/2000\n",
      "7110/7110 [==============================] - 15s 2ms/step - loss: 9.1173 - acc: 0.0105\n",
      "Epoch 104/2000\n",
      "7110/7110 [==============================] - 15s 2ms/step - loss: 9.1173 - acc: 0.0110\n",
      "Epoch 105/2000\n",
      "7110/7110 [==============================] - 15s 2ms/step - loss: 9.1176 - acc: 0.0107\n",
      "Epoch 106/2000\n",
      "7110/7110 [==============================] - 13s 2ms/step - loss: 9.1174 - acc: 0.0097\n",
      "Epoch 107/2000\n",
      "7110/7110 [==============================] - 13s 2ms/step - loss: 9.1166 - acc: 0.0115\n",
      "Epoch 108/2000\n",
      "7110/7110 [==============================] - 11s 2ms/step - loss: 9.1162 - acc: 0.0101\n",
      "Epoch 109/2000\n",
      "7110/7110 [==============================] - 10s 1ms/step - loss: 9.1166 - acc: 0.0113\n",
      "Epoch 110/2000\n",
      "7110/7110 [==============================] - 13s 2ms/step - loss: 9.1167 - acc: 0.0104\n",
      "Epoch 111/2000\n",
      "7110/7110 [==============================] - 12s 2ms/step - loss: 9.1162 - acc: 0.0111\n",
      "Epoch 112/2000\n",
      "4480/7110 [=================>............] - ETA: 5s - loss: 9.0999 - acc: 0.0125"
     ]
    }
   ],
   "source": [
    "model.fit(dataset_x, dataset_y, epochs=2000, batch_size=64, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(dataset_x, dataset_y, batch_size=128)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 128)\n",
      "Start:  0.44285107 [25] End:  0.54137677 [91]\n"
     ]
    }
   ],
   "source": [
    "output = model.predict(np.reshape(dataset_x[0], (1, 7, 8, 8)))\n",
    "output = output[0]\n",
    "val1 = np.amax(output[:64])\n",
    "val2 = np.amax(output[64:])\n",
    "idx1 = np.where(output == val1)[0]\n",
    "idx2 = np.where(output == val2)[0]\n",
    "\n",
    "print('Start: ', val1, idx1, 'End: ', val2, idx2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "(28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "'''Trains a simple convnet on the MNIST dataset.\n",
    "Gets to 99.25% test accuracy after 12 epochs\n",
    "(there is still a lot of margin for parameter tuning).\n",
    "16 seconds per epoch on a GRID K520 GPU.\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 12\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "print(input_shape)\n",
    "\n",
    "# # convert class vectors to binary class matrices\n",
    "# y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "# y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "#                  activation='relu',\n",
    "#                  input_shape=input_shape))\n",
    "# model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# model.add(Dropout(0.25))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "#               optimizer=keras.optimizers.Adadelta(),\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# model.fit(x_train, y_train,\n",
    "#           batch_size=batch_size,\n",
    "#           epochs=epochs,\n",
    "#           verbose=1,\n",
    "#           validation_data=(x_test, y_test))\n",
    "# score = model.evaluate(x_test, y_test, verbose=0)\n",
    "# print('Test loss:', score[0])\n",
    "# print('Test accuracy:', score[1])\n",
    "\n",
    "# # print(x_train[0])\n",
    "# # print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
